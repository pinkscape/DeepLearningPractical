{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Nueral Networks\n",
    "This week we will implement the backward propogation for different layers in a convloutional neural networks in. All the coding task will take place in \"**cnn/layers.py**\". We provide the implmentation for forward propogation, and the skeleton & instruction for the backward propogation.\n",
    "\n",
    "* Task 0: Implement the backward pass for the fully connected layer\n",
    "* Task 1: Implement the backward pass for the Convolutional layer with forloop\n",
    "* Task 3: Implement the backward pass for the Max pooling layer with forloop\n",
    "* Task 3: Implement backward pass for the Relu layer\n",
    "* Task 4: Try fast implement for Convolution and Max pooling layers. Make sure you understand how it works.\n",
    "\n",
    "** FIRST: create a new folder called \"notebooks/cnn\", unzip these files into it. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As usual, Here is a bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from notebooks.utils.data_utils import load_CIFAR10\n",
    "from scipy.misc import imread, imresize\n",
    "from notebooks.utils.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cnn.layers import *\n",
    "from cnn.fast_layers import *\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def get_CIFAR10_data(num_training=4900, num_validation=100, num_test=100):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    print('Loading CIFAR10 ...')\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = '../opt/data/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    \n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0: Implement backward pass for Fully Connected Layer\n",
    "The `affine_forward` function in `cnn/layers.py` gives an implementation for the forward pass. Make sure you understand how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (4, 32)\n",
      "w shape:  (32, 2)\n",
      "b shape:  (2,)\n",
      "output shape:  (4, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 32)\n",
    "w = np.random.randn(32, 2)\n",
    "b = np.random.randn(2,)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "print 'x shape: ', x.shape\n",
    "print 'w shape: ', w.shape\n",
    "print 'b shape: ', b.shape\n",
    "print 'output shape: ' , out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Implement the affine_backward function in cnn/layers.py. **\n",
    "\n",
    "When it is done, run the following code to check your backward pass with a numeric gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function\n",
      "dx error:  2.49551962219e-09\n",
      "dw error:  1.00019347521e-09\n",
      "db error:  1.04012668057e-10\n"
     ]
    }
   ],
   "source": [
    "dout = np.random.randn(4, 2)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "out, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# Your errors should be around 1e-9'\n",
    "print 'Testing affine_backward function'\n",
    "print 'dx error: ', rel_error(dx, dx_num)\n",
    "print 'dw error: ', rel_error(dw, dw_num)\n",
    "print 'db error: ', rel_error(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Implement backward pass for Convolution Layer\n",
    "The core of a convolutional network is the convolution operation. The `conv_forward_naive` function in `cnn/layers.py` gives a forloop implementation for the forward pass. Make sure you understand how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (2, 3, 4, 4)\n",
      "w shape:  (3, 3, 4, 4)\n",
      "b shape:  (3,)\n",
      "output shape:  (2, 3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "print 'x shape: ', x.shape\n",
    "print 'w shape: ', w.shape\n",
    "print 'b shape: ', b.shape\n",
    "print 'output shape: ' , out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the `conv_backward_naive` function in `cnn/layers.py`**. with forloops. \n",
    "\n",
    "When it is done, run the following code to check your backward pass with a numeric gradient check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "dx error:  6.39170218136e-10\n",
      "dw error:  1.09929261977e-09\n",
      "db error:  3.27628178591e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "F, C, HH, WW = w.shape\n",
    "N, _, H, W = x.shape \n",
    "dout = np.random.randn(4, 2, 1 + (W + 2 * conv_param['pad'] - WW) / conv_param['stride'], 1 + (W + 2 * conv_param['pad'] - WW) / conv_param['stride'])\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around 1e-9'\n",
    "print 'Testing conv_backward_naive function'\n",
    "print 'dx error: ', rel_error(dx, dx_num)\n",
    "print 'dw error: ', rel_error(dw, dw_num)\n",
    "print 'db error: ', rel_error(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Implement backward pass for Max pooling\n",
    "The max_pooling_forward_naive function in cnn/layers.py gives a forloop implementation for max pooling layer. Make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (2, 2, 2, 2)\n",
      "output shape: (2, 2, 1, 1)\n",
      "x : [[[[-0.3        -0.25333333]\n",
      "   [-0.20666667 -0.16      ]]\n",
      "\n",
      "  [[-0.11333333 -0.06666667]\n",
      "   [-0.02        0.02666667]]]\n",
      "\n",
      "\n",
      " [[[ 0.07333333  0.12      ]\n",
      "   [ 0.16666667  0.21333333]]\n",
      "\n",
      "  [[ 0.26        0.30666667]\n",
      "   [ 0.35333333  0.4       ]]]]\n",
      "output:  [[[[-0.16      ]]\n",
      "\n",
      "  [[ 0.02666667]]]\n",
      "\n",
      "\n",
      " [[[ 0.21333333]]\n",
      "\n",
      "  [[ 0.4       ]]]]\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 2, 2, 2)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "print 'x shape:' , x.shape\n",
    "print 'output shape:' , out.shape\n",
    "print 'x :' , x\n",
    "print 'output: ' , out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the `max_pool_backward_naive` function in the file `cnn/layers.py`** with forloop implementation here.\n",
    "\n",
    "When it is done, run the following code to check your backward pass with a numeric gradient check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward_naive function:\n",
      "dx error:  3.27563083096e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_naive(x, pool_param)\n",
    "dx = max_pool_backward_naive(dout, cache)\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print 'Testing max_pool_backward_naive function:'\n",
    "print 'dx error: ', rel_error(dx, dx_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Implement backward pass for Relu \n",
    "\n",
    "The relu_forward function in cnn/layers.py gives a forloop implementation for the Relu layer. Make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (1, 1, 2, 2)\n",
      "output shape: (1, 1, 2, 2)\n",
      "x : [[[[-0.3 -0.1]\n",
      "   [ 0.1  0.3]]]]\n",
      "output:  [[[[-0.  -0. ]\n",
      "   [ 0.1  0.3]]]]\n"
     ]
    }
   ],
   "source": [
    "x_shape = (1, 1, 2, 2)\n",
    "x = np.linspace(-0.3, 0.3, num=np.prod(x_shape)).reshape(x_shape)\n",
    "out, _ = relu_forward(x)\n",
    "\n",
    "print 'x shape:' , x.shape\n",
    "print 'output shape:' , out.shape\n",
    "print 'x :' , x\n",
    "print 'output: ' , out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the `relu_backward` function in `cnn/layers.py`**. \n",
    "\n",
    "When it is done, run the following code to check your backward pass with a numeric gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (1, 1, 2, 2)\n",
      "output shape: (1, 1, 2, 2)\n",
      "x : [[[[-0.40420583 -0.57655132]\n",
      "   [ 1.12101946 -0.33497201]]]]\n",
      "output:  [[[[-0.         -0.        ]\n",
      "   [ 1.12101946 -0.        ]]]]\n",
      "dx:  [[[[-0.         -0.        ]\n",
      "   [ 1.74034053  0.        ]]]]\n",
      "Testing relu_backward function:\n",
      "dx error:  3.27560157221e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(1, 1, 2, 2)\n",
    "dout = np.random.randn(1, 1, 2, 2)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "out, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "print 'x shape:' , x.shape\n",
    "print 'output shape:' , out.shape\n",
    "print 'x :' , x\n",
    "print 'output: ' , out\n",
    "print 'dx: ' , dx\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print 'Testing relu_backward function:'\n",
    "print 'dx error: ', rel_error(dx, dx_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Try Fast layer implementation\n",
    "Making convolution and pooling layers fast can be challenging. We provided fast implementations of the forward and backward passes for convolution and pooling layers in the file `cnn/fast_layers.py`.\n",
    "\n",
    "Run the follwing code to compare the speed of the naive implementation and the fast implementation. The fast convolution implementation depends on a Cython extension. **You need to compile it**:\n",
    "* open a terminal with the \"New\" drop list in Jupyter (you can find it at the top right corner at http://localhost:8888).\n",
    "* you run the following from the `cnn` directory.\n",
    "\n",
    "```bash\n",
    "python setup.py build_ext --inplace\n",
    "```\n",
    "\n",
    "**Understand how \"conv_forward_fast\" and \"conv_backward_fast\" are implmented in cnn/fast_layers.py.** The key for this faster implementation is to treat the convolutional layer as a fully connected layer. This is done by calling \"im2col\" to create a data matrix, where each column is the receptive field of a single convolution operation. \n",
    "\n",
    "Check this link for more details: https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/making_faster.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_fast:\n",
      "Naive: 2.821036s\n",
      "Fast: 0.011230s\n",
      "Speedup: 251.205511x\n",
      "Difference:  3.37535377987e-11\n",
      "\n",
      "Testing conv_backward_fast:\n",
      "Naive: 3.846264s\n",
      "Fast: 0.010529s\n",
      "Speedup: 365.300507x\n",
      "dx difference:  6.5734582088e-11\n",
      "dw difference:  3.80895206143e-13\n",
      "db difference:  1.27041950307e-14\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(100, 3, 31, 31)\n",
    "w = np.random.randn(25, 3, 3, 3)\n",
    "b = np.random.randn(25,)\n",
    "dout = np.random.randn(100, 25, 16, 16)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = conv_forward_naive(x, w, b, conv_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)\n",
    "t2 = time()\n",
    "\n",
    "print 'Testing conv_forward_fast:'\n",
    "print 'Naive: %fs' % (t1 - t0)\n",
    "print 'Fast: %fs' % (t2 - t1)\n",
    "print 'Speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'Difference: ', rel_error(out_naive, out_fast)\n",
    "\n",
    "t0 = time()\n",
    "dx_naive, dw_naive, db_naive = conv_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast, dw_fast, db_fast = conv_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print '\\nTesting conv_backward_fast:'\n",
    "print 'Naive: %fs' % (t1 - t0)\n",
    "print 'Fast: %fs' % (t2 - t1)\n",
    "print 'Speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'dx difference: ', rel_error(dx_naive, dx_fast)\n",
    "print 'dw difference: ', rel_error(dw_naive, dw_fast)\n",
    "print 'db difference: ', rel_error(db_naive, db_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pool_forward_fast:\n",
      "Naive: 0.183794s\n",
      "fast: 0.003515s\n",
      "speedup: 52.288408x\n",
      "difference:  0.0\n",
      "\n",
      "Testing pool_backward_fast:\n",
      "Naive: 0.557820s\n",
      "speedup: 57.560632x\n",
      "dx difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(100, 3, 32, 32)\n",
    "dout = np.random.randn(100, 3, 16, 16)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = max_pool_forward_naive(x, pool_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = max_pool_forward_fast(x, pool_param)\n",
    "t2 = time()\n",
    "\n",
    "print 'Testing pool_forward_fast:'\n",
    "print 'Naive: %fs' % (t1 - t0)\n",
    "print 'fast: %fs' % (t2 - t1)\n",
    "print 'speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'difference: ', rel_error(out_naive, out_fast)\n",
    "\n",
    "t0 = time()\n",
    "dx_naive = max_pool_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast = max_pool_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print '\\nTesting pool_backward_fast:'\n",
    "print 'Naive: %fs' % (t1 - t0)\n",
    "print 'speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'dx difference: ', rel_error(dx_naive, dx_fast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}